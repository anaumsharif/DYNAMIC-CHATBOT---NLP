{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29970c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string #punctuations, data preprocessing\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #data encoding\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer #5th step - data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449f79e",
   "metadata": {},
   "source": [
    "Function for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb9765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize():\n",
    "    file = open('corpus.txt','r',errors='ignore')\n",
    "    corpus = file.read()\n",
    "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
    "    word_tokens = nltk.word_tokenize(corpus)\n",
    "    return sentence_tokens, word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e9550",
   "metadata": {},
   "source": [
    "Function for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c911b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a24a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemtokens(tokens):\n",
    "    lst=[] \n",
    "    for i in tokens: #every individual token have to be lemmatized\n",
    "        lst.append(lemmatizer.lemmatize(i))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c163a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove the punctuation\n",
    "punct = dict((ord(i),None) for i in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04ba22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmer(text):\n",
    "    #Tokenize\n",
    "    tokenized_text=nltk.word_tokenize(text.lower().translate(punct))\n",
    "    lemmatized_values = lemtokens(tokenized_text)\n",
    "    return lemmatized_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2b3ec",
   "metadata": {},
   "source": [
    "Function for Greeting - Rule Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28dd5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_inputs = ['hello','hi','hey','greetings']\n",
    "greeting_responses = ['I am a chatbot','hello','hi','whats up','hi there']\n",
    "\n",
    "def greeting(text):\n",
    "    #Tokenize\n",
    "    for token in text.split():\n",
    "        #inputs\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2efb1",
   "metadata": {},
   "source": [
    "tfidf = [t1,t2,t3,t4,user_query]\n",
    "\n",
    "sim = [s1,s2,s3,s4,s5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5234b572",
   "metadata": {},
   "source": [
    "Function to generate response for the queries from the corpus!\n",
    "\n",
    "Data Encoding - TFIDF\n",
    "Similarity Metric - cosine similarity\n",
    "choosing the vector with maximum similarity in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5077e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(user_query):\n",
    "    bot_response=''\n",
    "    \n",
    "    #Tokenize\n",
    "    sent_tokens,word_tokens = tokenize()\n",
    "    sent_tokens.append(user_query)\n",
    "    \n",
    "    #Vectorizing\n",
    "    tfidf_obj = TfidfVectorizer(tokenizer=lemmer,stop_words=\"english\")\n",
    "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
    "    \n",
    "    #cosine similarity\n",
    "    sim_values = cosine_similarity(tfidf[-1],tfidf) #cosine similarity of the last element with the entire list\n",
    "    \n",
    "    #selecting the response or token with max similarity\n",
    "    index = sim_values.argsort()[0][-2]\n",
    "    \n",
    "    flattened_sim = sim_values.flatten()\n",
    "    flattened_sim.sort()\n",
    "    \n",
    "    required_tfidf = flattened_sim[-2]\n",
    "    \n",
    "    if(required_tfidf==0):\n",
    "        bot_response += 'I cannot understand'\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response += bot_response + sent_tokens[index]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0752f8",
   "metadata": {},
   "source": [
    "main() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b460fb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot\n",
      "what is deep learning\n",
      "Bot:  Deep learning is a subset of machine learning, which involves the use of neural networks to process and analyze large amounts of data.\n",
      "types of ai\n",
      "Bot:  There are several different types of AI, each with their own strengths and weaknesses.\n",
      "negative impacts of ai\n",
      "Bot:  Despite the many benefits of AI, there are also concerns about its potential negative impacts.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3596\\2893160762.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0muser_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0muser_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_query\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             )\n\u001b[1;32m-> 1177\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1217\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1219\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1220\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot\")\n",
    "flag = 1\n",
    "\n",
    "while(flag == 1):\n",
    "    user_query = input()\n",
    "    user_query = user_query.lower()\n",
    "    \n",
    "    #exit\n",
    "    if(user_query=='exit'):\n",
    "        flag=0\n",
    "        print('Bot: Bye C u!')\n",
    "    \n",
    "    else:\n",
    "        #greeting\n",
    "        if(greeting(user_query) != None):\n",
    "            print(\"Bot: \"+greeting(user_query))\n",
    "        \n",
    "        #general\n",
    "        else:\n",
    "            res = respond(user_query)\n",
    "            print(\"Bot: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d8ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
